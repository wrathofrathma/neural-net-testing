#!/usr/bin/env python

import cupy as cp
import numpy as np
from cupy import array
from cupy import matmul
from cupy import stack

# For our activation function
from cupy import tanh

# For model loading/saving
import json



# ACTIVATION FUNCTIONS
# Input - cp.array containing the pre-activation values
# Output - cp.array containing the activation values
def tanh_activation(z):
    return cp.tanh(z)

# Derivative of tanh activation
def dtanh(z):
    # print("Tanh derivative: " + str(z))
    return cp.power(cp.cosh(z),2)

def relu(z):
    zeros = cp.zeros(len(z))
    return cp.maximum(zeros,z)

# Derivative of relu
# f'(x) = { 0 for x < 0
#           1 for x >= 1 }
def drelu(z):
    r = z[:]
    for i in range(0,len(r)):
        if(r[i]>=0):
            r[i] = 1
        else:
            r[i] = 0
    return r
# def softmax(z):
#     return cp.exp(z) / float(sum(cp.exp(z)))

# def dsoftmax(z):
#     s = z.reshape(-1,1)
#     return cp.diagflat(s) - cp.dot(s, s.T)
# mean squared error is going to be our default cost function/error function to minimize. It derives easily and I don't feel like doing too much extra math
def mse(predicted, actual):
    # print("mse")
    # print(cp.power(predicted-actual,2))
    return cp.power(predicted-actual,2)/2
def cmse(predicted, actual):
    # print("cmse")
    # print(predicted)
    # print(actual)
    # print(cp.sum(mse(predicted,actual)) / len(predicted))
    return cp.sum(mse(predicted, actual))/len(predicted)
def dmse(predicted, actual):
    return (predicted-actual)

# LOSS FUNCTION?????
def loss_placeholder():
    pass

# Placeholder object to represent a node in a layer.
class Node:
    def __init__(self, out_size=0):
        self.set_output_size(out_size)
        self.bias = cp.random.random()

    def set_output_size(self, size):
        self.out_size = size
        self.weights = cp.random.rand(size)

    def set_bias(self, bias):
        self.bias = bias

    def get_bias(self):
        return self.bias

    # Returns the row matrix vector of the weights to the next layer's nodes.
    def get_weights(self):
        return self.weights
    def set_weights(self, weights):
        self.weights = weights

# Layer object - Contains a number of nodes
# Houses vectorized bias and weights for the layer.
class Layer:

    def set_activation(self, activation):
        self.activation = activation
        if(activation=='tanh'):
            self.activation_function = tanh_activation
            self.dact = dtanh
        elif(activation=='softmax'):
            self.activation_function = softmax
        else:
            self.activation_function = relu
            self.dact = drelu

    def get_weights(self):
        return self.weights

    def get_biases(self):
        return self.biases

    def set_output_size(self, n):
        self.n_out = n
        # Generate weights
        self.weights = cp.random.rand(self.n_nodes, self.n_out)

    def __init__(self, n_nodes, n_out=1, activation='relu', is_first=False):
        self.set_activation(activation)
        self.n_nodes = n_nodes
        self.is_first=is_first
        self.n_out = n_out
        self.weights = cp.random.rand(n_nodes, n_out)
        self.biases = cp.ones(n_nodes)


    def set_weights(self, weights):
        self.weights = weights

    def set_biases(self, biases):
        self.biases = biases

    # An = Wn-1 * An-1 + bn
    # This function creates pre-activation vector for the layer. Which is the weight matrix of the previous layer, multipled by the activations of the previous layer, plus the current layer's bias vector.
    # Thus the input we need is defined below

    # Input: data - Activation data from previous layer. Should be a cupy.array or cupy.stack
    # Input: weights - Weights from previous layer. Must be a cupy.stack
    # Return: pre-activation z vector
    def get_preactivation_vector(self, data, weights=None):
        biases = self.get_biases() # Bias vector
        biases = stack(biases)
        if(self.is_first):
            z = data
        else:
            z = cp.dot(data, weights)
        # z += biases
        return z

    # This function applies whatever activation method is used for this layer, to the z vector generated by get_preactivation_vector
    # Returns a cupy.stack column matrix of this layer's output activations.
    def get_activations(self, z):
        # acts = []
        # for a in z:
        #     acts.append(self.activation_function(a))
        # acts = stack(acts)
        # return acts
        return self.activation_function(z)

class Model:
    # Initializes our input/output dimensions and layer data.
    # Also adds the first layer for the input layer
    def __init__(self, input_size, output_size, learn_rate=1e-3, error='mse', loss='placeholder', decay_rate=1e-5, decay_epochs=10):
        self.layers = []
        self.set_error(error)
        self.input_size = input_size
        self.output_size = output_size
        self.num_layers = 0
        self.__create_input_layer()
        self.learn_rate = learn_rate
        self.loss=loss
        self.decay_epochs=decay_epochs
        self.decay_rate=decay_rate
        if(loss=='placeholder'):
            self.loss_function=loss_placeholder

    # Sets the error function
    def set_error(self,error):
        self.error = error
        if(error=='mse'):
            self.error_function = mse
            self.cost_function = cmse
            self.dcost = dmse
        else:
            self.error_function = mse
            self.cost_function = cmse
            self.dcost = dmse

    # Creates the actual input layer.
    def __create_input_layer(self):
        input_layer = Layer(self.input_size, activation='tanh', is_first=True)
        self.layers.append(input_layer)
        self.num_layers+=1

    def add_layer(self, layer):
        size=layer.n_nodes
        self.layers[-1].set_output_size(size)
        self.layers.append(layer)
        self.num_layers+=1

    # Saves the model to disk in json format
    def save_to_disk(self, filename):
        model = {
            'input_size' : self.input_size,
            'output_size' : self.output_size,
            'learn_rate' : self.learn_rate,
            'loss' : self.loss,
            'error' : self.error,
            'layers' : None,
            'decay_rate' : self.decay_rate,
            'decay_epochs' : self.decay_epochs
        }
        layer_num = 0
        json_layers = []
        for layer in self.layers:
            weights = layer.get_weights().tolist()
            biases = layer.get_biases()
            biases = stack(biases)
            biases = biases.tolist()

            l = { '_layer_num' : layer_num,
                  'n_nodes' : layer.n_nodes,
                'activation' : layer.activation,
                 'weights' : weights,
                 'biases' : biases
                 }
            json_layers.append(l)
            layer_num+=1
        model['layers'] = json_layers
        json.dump(model, open(filename, "w"),sort_keys=True, indent=4)

    # Loads up a model format & weights from disk from json format
    def load_from_disk(filename):
        print("Loading model from filename: " + filename)
        json_model = json.load(open(filename, "r"))
        model = Model(json_model['input_size'], json_model['output_size'], learn_rate=json_model['learn_rate'],
                      loss=json_model['loss'], error=json_model['error'], decay_rate=json_model['decay_rate'], decay_epochs=json_model['decay_epochs'])
        layers = json_model['layers']
        # One pass to create the structure
        for i in range(0, len(layers)):
            if(i!=0):
                l = Layer(layers[i]['n_nodes'], activation=layers[i]['activation'])
                model.add_layer(l)
            else:
                model.layers[0].n_nodes = layers[0]['n_nodes']
                model.layers[0].set_activation(layers[0]['activation'])

        # Second pass to set weights & biases
        for i in range(0, len(layers)):
            weights = layers[i]['weights']
            biases = layers[i]['biases']
            for j in range(0, len(weights)):
                weights[j] = array(weights[j])
                biases[j] = array(biases[j])
            weights = stack(weights)
            model.layers[i].set_weights(weights)
            model.layers[i].set_biases(biases)
        return model

    # Recursive backwards propogation algorithm
    # lc - current layer
    # lp - previous layer
    # activations - activation matrix of every layer
    # zs - input to every activation function
    # y - expected output
    # dEdxj - previous layer's dE/dx values
    def backprop(self, lc, lp, activations, zs, y, dEdxj=None):
        if(lc==0): # TODO later, change htis to maybe do biases for input layer?
            return
        dact_func = self.layers[lc].dact
        wl = self.layers[lp].get_weights() # layer weights
        bl = self.layers[lc].get_biases() # layer biases
        zl = zs[lc] # zL of the current layer
        al = activations[lc] # activations of the current layer


        if(lc==self.num_layers-1):
            # print("Last layer")
            hidden=False
        else:
            # print("Hiddden")
            hidden=True

        # Calculate delta for the layer
        if(hidden):
            # Hidden layers
            # print("Hidden layers")
            # Need to calc dEdy differently.
            # For each node in layer i
            # - Sum of (wij * dE/xj) 
            # dEdxj is coming in as a column vector of the previous layer's dEdx values.
            # We can probably use matrix multiplication to apply this sum...
            # I think we can transpose the weight matrix for this. Since our normal weight matrix represents weights leaving node i, as a row,  
            # The normal dot product between a column vector and this should be what we desire 
            dedxj = dEdxj # irrelevant statement, but just here to show that it exists. The previous layer's dEdx

            wc = self.layers[lc].get_weights()  # weights of the current layer. Each row = weights leaving a node.
            # print("dedxj: " + str(dEdxj))
            # print("Wc: " )
            # print(wc)
            # print(wc.shape)
            dEdy = []
            for row in wc:
                dEdy.append(cp.sum(row*dEdxj))
            dEdy = cp.stack(dEdy)
            # print("dEdy: " + str(dEdy))
            # dEdy = cp.dot(dEdxj, wc) # new dEdy
            dEdx = dact_func(zl) * dEdy
            # print("dE/dx = d/dx activation(x) * dE/dy " + str(dEdx))
            yp = activations[lc] # activations of the previous layer
            # print("dE/dY: " + str(dEdy))
            #

            # print("Input of layer: " + str(zl))
            # print("Activations of layer: "+str(al))
            # print("dE/dY: " + str(dEdy))
            # # Also need dE/dX for how the error changes based on the input from every node
            # print("dact: " + str(dact_func(zl)))
            # print("dEdx = dact(x) * dEdy = " + str(dEdx))

            # yp = yp.reshape(1,yp.shape[0])
            # dEdx = dEdx.reshape(dEdx.shape[0],1)
            dEdW = []
            for dedx in dEdx:
                dEdW.append(array(dedx * yp))
            dEdW = cp.stack(dEdW)
            # dEdW = cp.matmul(dEdx, yp) # our adjustment matrix
            dEdW = dEdW.transpose()
            #exit(1)
            #
        else:
            # If it's not hidden, then it's the output layer so our delta looks something like
            # delta = dcost * dact
            # dbias = delta
            # dweights = delta * prev_acts
            # print("dc")
            # print("al: " + str(al))
            # print("y : " + str(y))
            # Need to find dE/dY for how the error changes based on the output from every node.
            dEdy = self.dcost(al,y) # Derivative of cost function with respect to the activations
            # print("Input of layer: " + str(zl))
            # print("Activations of layer: "+str(al))
            # print("Expected activation: " + str(y))
            # print("dE/dY: " + str(dEdy))
            # Also need dE/dX for how the error changes based on the input from every node
            # print("dact: " + str(dact_func(zl)))
            dEdx = dact_func(zl) * dEdy
            # print("dEdx = dact(x) * dEdy = " + str(dEdx))
           # print("dE/dx = d/dx activation(x) * dE/dy " + str(dEdx))
            yp = activations[lp] # activations of the previous layer

            # For each node in layer j
            # - Calc the dE/dW going into that node based on the dEdx of the node and the dEdy of all of the nodes in layer i
            # - Weigths going into a node are represented as a row in our dEdW matrix. So weights going into node 0  of layer j, would be in row 0. 
            # - We already have the dEdx vector and dEdys I think.
            # So for node 0 of row j, the dE/dW vector would be
            # [ y0 * dE/dx0 , y1 * dE/dx0, y2 * dE/dx0 ] where ys are in layer i and dE/dx are in layer j
            # [ y0 * dE/dx1 , y1 * dE/dx1, y2 * dE/dx1 ] for the next node.
            # [ y0 * dE/dx2 , y1 * dE/dx2, y2 * dE/dx2 ] for the last node. (my training example contains 3 nodes in each layer.)

            # If we do normal matrix multiplication, we'll end up with a bunch of dot products, when what we want are the scalar multiples on the inside.
            # So let's create our wegiht matrix by multiplying the outputs by our dE/dx 
            # For each error with respect to the input in layer j, let's multiply it by the outputs of the previous layer
            dEdW = []
            for dedx in dEdx:
                dEdW.append(array(dedx * yp))
            dEdW = cp.stack(dEdW)
            # This should give us a structure similar to what we listed above.
            # We need to transpose the adjustment matrix because our normal weight matrix has rows assigned to weights leaving a node in layer i.
            # Each column corresponds with the weights being applied to a node's input on layer j
            # Which is flipped from how we generated the dEdw
            dEdW = dEdW.transpose()
        # print("YP")
        # print(yp)
        # print("dEdW")
        # print(dEdW)
        # print("weights")
        # print(wl)
        # print("LR * dEdW")
        # print(self.learn_rate * dEdW)
        new_weights = (wl - self.learn_rate*dEdW)*0.99
        # print("new weights")
        # print(new_weights)
        db = dEdx.reshape(dEdx.shape[0],)
        self.layers[lp].weights = new_weights
        # print("New weights: " + str(new_weights))
        #self.layers[lc].biases = bl - self.learn_rate*db
        self.backprop(lc-1, lp-1, activations, zs, y, dEdx)
    # Function to train for one epoch over a dataset to match the expected outputs.
    def train(self, dataset, outputs, epochs=10, verbose=False):
        # print("Input training data")
        # print(dataset)
        # print("Actual output data")
        # print(outputs)

        # Since cupy stacks have shapes, we can use those to verify whether our data fits the model we've designed
        n_dsequences = dataset.shape[0]
        n_osequences = outputs.shape[0]

        osize = outputs.shape[1]
        dsize = dataset.shape[1]

        assert (n_dsequences==n_osequences), "Number of sequences in dataset must match number of actual output sequences"
        assert (osize==self.output_size), "Output training data must be same dimensions as last layer output"
        assert (dsize==self.input_size), "Input training data must be same dimensions of input layer"

        # print("Datset & output shapes: " + str(dataset.shape) + " " + str(outputs.shape))
        batch_errors = []
        n_layers = len(self.layers)
        for e in range(0,epochs):
            errors=[]
            # For each training example
            for i in range(0, len(dataset)):
                # print("In training loop")
                data = cp.copy(dataset[i]) # training example
                out = outputs[i] # Expected output
                # Feed the data forward and store the activations
                # print("Data: " + str(data))
                zs, acts = self.feedforward(data)
                # print("zs: " + str(zs))
                # print("acts: " + str(acts))
                # Create error vector
                errors.append(self.cost_function(acts[-1],out))
                batch_errors.append(self.cost_function(acts[-1],out))
                error = self.error_function(acts[-1], out)

                # print("Before back propagation")
                #Backwards propagate with the computed error, activations, and goal output to get dw, db
                self.backprop(n_layers-1, n_layers-2, acts, zs, out)
                if(i%600):
                    print("Out: " + str(zs[-1]) + " | " + "Actual: " + str(out) +  " | Error: " + str(cp.sum(error)))
            errors = cp.stack(errors)
            epoch_error = cp.sum(errors) / len(errors)
            # print(errors)
            if(verbose):
                print("Total epoch error: " + str(epoch_error))
        batch_errors = cp.stack(batch_errors)
        batch_errors = cp.sum(batch_errors) / len(batch_errors)
        print("Average batch error: " + str(batch_errors))

    def feedforward(self, data):
        x = cp.copy(data)
        weights = None
        zs = []
        acts = []
        # For each layer, we need the previous layer's activation data and we need the weights of the previous layer.
        count=0
        for layer in self.layers:
            # print("Layer: " + str(count))
            count+=1
            z=layer.get_preactivation_vector(x, weights)
            z=cp.clip(z, 1e-10, 0.9999999)
            zs.append(z)
            x = layer.get_activations(z) # Next layer's input
            x = cp.clip(x, 1e-10, 0.99999999)
            acts.append(x)
            weights = layer.get_weights()
        return zs,acts
    
    def predict(self, data):
        zs, acts = self.feedforward(data)
        return acts[-1]
    def get_model_info(self):
        info = {}
        info['cost'] = self.error
        info['learn_rate'] = self.learn_rate
        info['decay_rate'] = self.decay_rate
        info['weight_decay'] = 0.99
        info['activation'] = self.layers[-1].activation
        info['input_size'] = self.input_size
        info['output_size'] = self.output_size
        info['n_layers'] = self.num_layers
        return info
# a = [array([0.1,0.1,1], dtype=cp.float32), array([0.1,1,1], dtype=cp.float32), array([1,1,1], dtype=cp.float32), array([0.1,0.1,0.1], dtype=cp.float32)]
# a = [array([0,0,1], dtype=cp.float32)]
# a = stack(a)
# o = [array([0,0,0], dtype=cp.float32), array([1,1,0], dtype=cp.float32), array([1,1,0], dtype=cp.float32), array([0,0,0], dtype=cp.float32)]
# o = [0,1,1,0]
# o = [1, 0]
#o = stack(o)
# o = array(o, dtype=cp.float32)
# o = o.reshape(4,3)
print("Loading mnist dataset")
from tensorflow.keras.datasets import mnist
(x_dataset, y_labels), (x_test, y_test) = mnist.load_data()
print("Normalizing the dataset")
x_dataset = x_dataset/255.0
print("Reshaping dataset")
x_dataset = x_dataset.reshape(60000, 784)
print("Converting to python list")
x_dataset = x_dataset.tolist()
print("Converting to cupy arrays")
x_dataset = cp.array(x_dataset, cp.float32)
print(x_dataset)
print("Converting to cupy stack")
x_dataset=cp.stack(x_dataset)
print(x_dataset)
print("Creating labels")
labels = []
for label in y_labels:
    y = cp.zeros(10)
    y[label] = 1
    labels.append(y)
print(labels[0])
print("labels shape")
labels = cp.stack(labels)
print(labels.shape)
print("data shape")
print(x_dataset.shape)
# m = Model.load_from_disk("test.m")
m = Model(784,10, learn_rate=1e-2, decay_rate=1e-5, decay_epochs=10)
l1 = Layer(784, activation='relu')
l2 = Layer(300, activation='relu')
l3 = Layer(10, activation='tanh')
m.add_layer(l1)
m.add_layer(l2)
m.add_layer(l3)
print("Model info")
print(m.get_model_info())

m.train(x_dataset, labels, epochs=1)
# print("Array input")
# print(a)
# print("Predicting before training with input: " + str(a[0]))
# pred = m.predict(a[0])
# print(pred)
# print("Expected output was: " + str(o[0]))


# m.train(a,o, epochs=1000, verbose=True)
# print("Predicting after 1,000 epochs with input: " + str(a[0]))
# pred = m.predict(a[0])
# print(pred.tolist())
# print("Expected output was: " + str(o[0]))
# pred = m.predict(a[0])
# print(pred.tolist())
# print("Expected output was: " + str(o[0]))
# m.save_to_disk("test.m")

# m = Model(4,4)
# l1 = Layer(4, activation='tanh')
# #l1 = Layer(4)
# # l2 = Layer(8,8)
# # l3 = Layer(5,5)
# # l4 = Layer(16,16)
# m.add_layer(l1)
# # m.add_layer(l2)
# # m.add_layer(l3)
# # m.add_layer(l4)
# l0 = m.layers[0]
# w = l1.get_weights()
# print(w)
# b = l1.get_biases()
# print(b)
# print("Feeding forward")
# print(m.feed_forward(a))

# m.save_to_disk("hello")

# model = Model.load_from_disk("hello")
# a = array(a)
# for l in model.layers:
#     print("Weights")
#     print(l.get_weights())
#     print("Biases")
#     print(l.get_biases())

# print(model.feed_forward(a))
# print("Manual pass")
# print("Input data: " + str(a))
# print("Preactivations & activations of l0")
# pv0 = l0.get_preactivation_vector(a, None)
# act0 = l0.get_activations(pv0)
# w0 = l0.get_weights()
# print(pv0)
# print(act0)
# print("Preactivations & activations of l1")
# pv1 = l1.get_preactivation_vector(act0, w0)
# act1 = l1.get_activations(pv1)
# print(pv1)
# print(act1)


#w = [0.1, 0.2, 0.3, 0.4, 0.2, 0.3, 0.4, 0.5, 0.3, 0.4, 0.5, 0.6, 0.7, 0.4, 0.5, 0.6]
#w = array(w)
#w = w.reshape(4,4)
#print("Weight Matrix")
#print(w)
#print("Activations")
#print(a)

#print("Output activations")
#print(matmul(w,a))
